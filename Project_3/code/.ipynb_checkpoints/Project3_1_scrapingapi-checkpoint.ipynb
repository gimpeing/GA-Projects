{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Web APIs & Classification\n",
    "### Notebook 1: Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem Statement\n",
    "\n",
    "Product review is widely available in this digital world. People tends to do research on product reviews prior to purchase. However, there is fake review which is written by professional to boost the sale of a particular product. Thus, how would the consumer able to differentiate true user's genuine review from fake review?\n",
    "\n",
    "We can use Natural Language Processing (NLP) to train a classifier to best predict or differentiate the two.\n",
    "\n",
    "To do this, content from two subreddits were collected to evaluate different classifier model (Logistic Regression and Naive Bayes) on this binary classification problem.\n",
    "Subreddits selected are:\n",
    "1. [nosleep](https://www.reddit.com/r/nosleep/)\n",
    "2. [Thetruthishere](https://www.reddit.com/r/Thetruthishere/)\n",
    "\n",
    "Reason for selecting the two subreddits are they are about horror 'story'. Content in `nosleep` is of made up horror stories , whereas `Thetruthishere` is true horror personal experience.\n",
    "\n",
    "Accuracy score is used to evaluate the success of the model as on how effective the model is able to differentiate post from the two subbredits.\n",
    "\n",
    "Once the model is able to classify and distinguish between these two subreddits, then it would be able to adapt similar approach to detect made up (fake) review of a product for consumer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content\n",
    "- [Data Collection](#3.-Data-Collection)\n",
    "- Data Cleaning and EDA\n",
    "- Preprocessing\n",
    "- Modeling\n",
    "- Model Evaluation\n",
    "- Conclusion and Recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project was divided into 3 notebooks:\n",
    "notebook 1: Data Collection\n",
    "notebook 2: Data Cleaning, EDA, Preprocessing\n",
    "notebook 3: Modeling, Evaluation, Conclusion and Recommendations\n",
    "\n",
    "**Data Collection** is done by webscraping using the `requests` library. By default, Reddit give 25 posts per request. To get enough data, I'll need to use a `for loop` to continuously scrap the data by including `time.sleep()` function at the end of each loop to allow for a break in between requests.\n",
    "\n",
    "**Data Cleaning and EDA**\n",
    "Cleaning involved removing duplicate post, removing post with null content. This is left with 834 posts from `nosleep` and 937 posts from `Thetruthishere`. It was observed that `nosleep` has much longer post content comnpared to `Thetruthishere`. Asides, CountVectorizer is used to find out the most frequent words appeared in the two subreddits.\n",
    "\n",
    "**Preprocessing**\n",
    "Pre-processing includes removing html, removing non-letters, lemmatizing, and removing stopwords. `Subreddit` column that indicate which subreddit the post originated from is converted into binary number by assigning `1` to `nosleep` and `0` to `Thetruthishere`. The data was divided into train set (75%) and test set (25%).\n",
    "\n",
    "**Modeling**\n",
    "Baseline score, which is the null model by predicting the majority class is defined.\n",
    "Baseline accuracy = *53%*\n",
    "\n",
    "|Target Variable|Normalized Counts|\n",
    "|---|---|\n",
    "|1|0.529678|\n",
    "|0|0.470322|\n",
    "\n",
    "*where 1 equals `nosleep`, 0 equals `Thetruthishere`*\n",
    "\n",
    "Two Vectorizer extraction techniques are used to transform the post's content (string of words) into numeric X matrix that is able to use for modeling are:\n",
    "- CountVectorizer\n",
    "- TfidfVectorizer\n",
    "\n",
    "For each Vectorizer, two classification models are built:\n",
    "- Multinomial Naive Bayes\n",
    "- Logistic Regression\n",
    "\n",
    "Model optimization was done by using GridSearchCV to identify the optimal hyperparameters and were built into the classificaiton models.\n",
    "\n",
    "**Model Evaluation**\n",
    "Accuracy score was used to evaluate how well the classification model perform. This is because there is no greater detriment to false positive (actual post is `Thetruthishere` but predict it came from `nosleep`'s subreddit).\n",
    "Generally, all models perform well with accuracy in the range of 92%-94%. They all outperform the baseline. New post from each subreddit were pulled to check how well the model generalize to unseen data. *Logistic Regression model using TfidfVectorizer* performs the best as it is able to predict equally well by having consistent accuracy score around **93%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "\n",
    "#To visualize the whole grid\n",
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Explore which items to scrap from reddits.com\n",
    "\n",
    "The two selected subreddits to perform webscraping are:\n",
    "1. nosleep\n",
    "2. the true is here\n",
    "\n",
    "Using the `requests` library to gather the data, i.e. post from reddits.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### url for the first reddit sub-post:\n",
    "url = 'https://www.reddit.com/r/nosleep.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because Reddit has throttled python's default user agent, I'll need to set a custom user-agent to get the requests to work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### custom user-agent\n",
    "headers = {'User-agent': 'Pony Inc 1.0'}\n",
    "res = requests.get(url, headers = headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### check the status, it returns 200, means it is okay\n",
    "res.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use res.json() to convert the response into a dictionary format and set this to a variable\n",
    "nosleep_dict = res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial exploration of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'kind']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1st layer of dict: It has two keys, 'kind' & 'data'\n",
    "sorted(nosleep_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['after', 'before', 'children', 'dist', 'modhash']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2nd layer of dict, for key 'data', it has 5 keys.\n",
    "# 'children' & 'after' are the two keys that I would like to scrap\n",
    "sorted(nosleep_dict['data'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'kind']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3rd layer of dict, it has another two keys, 'kind' & 'data'\n",
    "# again, the 'data' has is the info that I will need\n",
    "sorted(nosleep_dict['data']['children'][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>approved_at_utc</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>saved</th>\n",
       "      <th>mod_reason_title</th>\n",
       "      <th>gilded</th>\n",
       "      <th>clicked</th>\n",
       "      <th>title</th>\n",
       "      <th>link_flair_richtext</th>\n",
       "      <th>subreddit_name_prefixed</th>\n",
       "      <th>hidden</th>\n",
       "      <th>pwls</th>\n",
       "      <th>link_flair_css_class</th>\n",
       "      <th>downs</th>\n",
       "      <th>hide_score</th>\n",
       "      <th>name</th>\n",
       "      <th>quarantine</th>\n",
       "      <th>link_flair_text_color</th>\n",
       "      <th>author_flair_background_color</th>\n",
       "      <th>subreddit_type</th>\n",
       "      <th>ups</th>\n",
       "      <th>total_awards_received</th>\n",
       "      <th>media_embed</th>\n",
       "      <th>author_flair_template_id</th>\n",
       "      <th>is_original_content</th>\n",
       "      <th>user_reports</th>\n",
       "      <th>secure_media</th>\n",
       "      <th>is_reddit_media_domain</th>\n",
       "      <th>is_meta</th>\n",
       "      <th>category</th>\n",
       "      <th>secure_media_embed</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>can_mod_post</th>\n",
       "      <th>score</th>\n",
       "      <th>approved_by</th>\n",
       "      <th>author_premium</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>edited</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>gildings</th>\n",
       "      <th>content_categories</th>\n",
       "      <th>is_self</th>\n",
       "      <th>mod_note</th>\n",
       "      <th>created</th>\n",
       "      <th>link_flair_type</th>\n",
       "      <th>wls</th>\n",
       "      <th>removed_by_category</th>\n",
       "      <th>banned_by</th>\n",
       "      <th>author_flair_type</th>\n",
       "      <th>domain</th>\n",
       "      <th>allow_live_comments</th>\n",
       "      <th>selftext_html</th>\n",
       "      <th>likes</th>\n",
       "      <th>suggested_sort</th>\n",
       "      <th>banned_at_utc</th>\n",
       "      <th>view_count</th>\n",
       "      <th>archived</th>\n",
       "      <th>no_follow</th>\n",
       "      <th>is_crosspostable</th>\n",
       "      <th>pinned</th>\n",
       "      <th>over_18</th>\n",
       "      <th>all_awardings</th>\n",
       "      <th>awarders</th>\n",
       "      <th>media_only</th>\n",
       "      <th>can_gild</th>\n",
       "      <th>spoiler</th>\n",
       "      <th>locked</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>visited</th>\n",
       "      <th>removed_by</th>\n",
       "      <th>num_reports</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>mod_reason_by</th>\n",
       "      <th>removal_reason</th>\n",
       "      <th>link_flair_background_color</th>\n",
       "      <th>id</th>\n",
       "      <th>is_robot_indexable</th>\n",
       "      <th>report_reasons</th>\n",
       "      <th>author</th>\n",
       "      <th>discussion_type</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>send_replies</th>\n",
       "      <th>whitelist_status</th>\n",
       "      <th>contest_mode</th>\n",
       "      <th>mod_reports</th>\n",
       "      <th>author_patreon_flair</th>\n",
       "      <th>author_flair_text_color</th>\n",
       "      <th>permalink</th>\n",
       "      <th>parent_whitelist_status</th>\n",
       "      <th>stickied</th>\n",
       "      <th>url</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>num_crossposts</th>\n",
       "      <th>media</th>\n",
       "      <th>is_video</th>\n",
       "      <th>author_cakeday</th>\n",
       "      <th>link_flair_template_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>nosleep</td>\n",
       "      <td></td>\n",
       "      <td>t2_c446v4f</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>February 2020 contest nominations</td>\n",
       "      <td>[]</td>\n",
       "      <td>r/nosleep</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_fdub8s</td>\n",
       "      <td>False</td>\n",
       "      <td>dark</td>\n",
       "      <td>None</td>\n",
       "      <td>public</td>\n",
       "      <td>86</td>\n",
       "      <td>1</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>86</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>{}</td>\n",
       "      <td>[writing]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>1.583439e+09</td>\n",
       "      <td>text</td>\n",
       "      <td>6</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>redd.it</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'giver_coin_reward': None, 'subreddit_id': N...</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>moderator</td>\n",
       "      <td>t5_2rm4d</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>fdub8s</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>TheCusterWolf</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/nosleep/comments/fdub8s/february_2020_conte...</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>True</td>\n",
       "      <td>https://redd.it/fduax3</td>\n",
       "      <td>13840304</td>\n",
       "      <td>1.583410e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>nosleep</td>\n",
       "      <td></td>\n",
       "      <td>t2_m297o</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>January 2020 Winners!</td>\n",
       "      <td>[]</td>\n",
       "      <td>r/nosleep</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_fecu80</td>\n",
       "      <td>False</td>\n",
       "      <td>dark</td>\n",
       "      <td>None</td>\n",
       "      <td>public</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>99</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>{}</td>\n",
       "      <td>[writing]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>1.583527e+09</td>\n",
       "      <td>text</td>\n",
       "      <td>6</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>redd.it</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>moderator</td>\n",
       "      <td>t5_2rm4d</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>fecu80</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>poppy_moonray</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/nosleep/comments/fecu80/january_2020_winners/</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>True</td>\n",
       "      <td>https://redd.it/fectho</td>\n",
       "      <td>13840304</td>\n",
       "      <td>1.583498e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>nosleep</td>\n",
       "      <td>42 years, 6 months and 3 days ago, on the 5th ...</td>\n",
       "      <td>t2_20gz4yg3</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>42 years ago we sent Voyager 1 into space to l...</td>\n",
       "      <td>[]</td>\n",
       "      <td>r/nosleep</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>t3_fgwmy3</td>\n",
       "      <td>False</td>\n",
       "      <td>dark</td>\n",
       "      <td>None</td>\n",
       "      <td>public</td>\n",
       "      <td>4933</td>\n",
       "      <td>1</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>4933</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'gid_1': 1}</td>\n",
       "      <td>[writing]</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>1.583960e+09</td>\n",
       "      <td>text</td>\n",
       "      <td>6</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>text</td>\n",
       "      <td>self.nosleep</td>\n",
       "      <td>True</td>\n",
       "      <td>&amp;lt;!-- SC_OFF --&amp;gt;&amp;lt;div class=\"md\"&amp;gt;&amp;lt...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'giver_coin_reward': None, 'subreddit_id': N...</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>t5_2rm4d</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>fgwmy3</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>RichardSaxon</td>\n",
       "      <td>None</td>\n",
       "      <td>202</td>\n",
       "      <td>True</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/nosleep/comments/fgwmy3/42_years_ago_we_sen...</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/nosleep/comments/fgwm...</td>\n",
       "      <td>13840304</td>\n",
       "      <td>1.583931e+09</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  approved_at_utc subreddit  \\\n",
       "0            None   nosleep   \n",
       "1            None   nosleep   \n",
       "2            None   nosleep   \n",
       "\n",
       "                                            selftext author_fullname  saved  \\\n",
       "0                                                         t2_c446v4f  False   \n",
       "1                                                           t2_m297o  False   \n",
       "2  42 years, 6 months and 3 days ago, on the 5th ...     t2_20gz4yg3  False   \n",
       "\n",
       "  mod_reason_title  gilded  clicked  \\\n",
       "0             None       0    False   \n",
       "1             None       0    False   \n",
       "2             None       0    False   \n",
       "\n",
       "                                               title link_flair_richtext  \\\n",
       "0                  February 2020 contest nominations                  []   \n",
       "1                              January 2020 Winners!                  []   \n",
       "2  42 years ago we sent Voyager 1 into space to l...                  []   \n",
       "\n",
       "  subreddit_name_prefixed  hidden  pwls link_flair_css_class  downs  \\\n",
       "0               r/nosleep   False     6                 None      0   \n",
       "1               r/nosleep   False     6                 None      0   \n",
       "2               r/nosleep   False     6                 None      0   \n",
       "\n",
       "   hide_score       name  quarantine link_flair_text_color  \\\n",
       "0       False  t3_fdub8s       False                  dark   \n",
       "1       False  t3_fecu80       False                  dark   \n",
       "2       False  t3_fgwmy3       False                  dark   \n",
       "\n",
       "  author_flair_background_color subreddit_type   ups  total_awards_received  \\\n",
       "0                          None         public    86                      1   \n",
       "1                          None         public    99                      0   \n",
       "2                          None         public  4933                      1   \n",
       "\n",
       "  media_embed author_flair_template_id  is_original_content user_reports  \\\n",
       "0          {}                     None                False           []   \n",
       "1          {}                     None                False           []   \n",
       "2          {}                     None                False           []   \n",
       "\n",
       "  secure_media  is_reddit_media_domain  is_meta category secure_media_embed  \\\n",
       "0         None                   False    False     None                 {}   \n",
       "1         None                   False    False     None                 {}   \n",
       "2         None                   False    False     None                 {}   \n",
       "\n",
       "  link_flair_text  can_mod_post  score approved_by  author_premium thumbnail  \\\n",
       "0            None         False     86        None           False             \n",
       "1            None         False     99        None           False             \n",
       "2            None         False   4933        None            True             \n",
       "\n",
       "  edited author_flair_css_class author_flair_richtext      gildings  \\\n",
       "0  False                   None                    []            {}   \n",
       "1  False                   None                    []            {}   \n",
       "2  False                   None                    []  {'gid_1': 1}   \n",
       "\n",
       "  content_categories  is_self mod_note       created link_flair_type  wls  \\\n",
       "0          [writing]    False     None  1.583439e+09            text    6   \n",
       "1          [writing]    False     None  1.583527e+09            text    6   \n",
       "2          [writing]     True     None  1.583960e+09            text    6   \n",
       "\n",
       "  removed_by_category banned_by author_flair_type        domain  \\\n",
       "0                None      None              text       redd.it   \n",
       "1                None      None              text       redd.it   \n",
       "2                None      None              text  self.nosleep   \n",
       "\n",
       "   allow_live_comments                                      selftext_html  \\\n",
       "0                False                                               None   \n",
       "1                 True                                               None   \n",
       "2                 True  &lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt...   \n",
       "\n",
       "  likes suggested_sort banned_at_utc view_count  archived  no_follow  \\\n",
       "0  None           None          None       None     False      False   \n",
       "1  None           None          None       None     False      False   \n",
       "2  None           None          None       None     False      False   \n",
       "\n",
       "   is_crosspostable  pinned  over_18  \\\n",
       "0             False   False    False   \n",
       "1             False   False    False   \n",
       "2             False   False    False   \n",
       "\n",
       "                                       all_awardings awarders  media_only  \\\n",
       "0  [{'giver_coin_reward': None, 'subreddit_id': N...       []       False   \n",
       "1                                                 []       []       False   \n",
       "2  [{'giver_coin_reward': None, 'subreddit_id': N...       []       False   \n",
       "\n",
       "   can_gild  spoiler  locked author_flair_text  visited removed_by  \\\n",
       "0     False    False    True              None    False       None   \n",
       "1     False    False    True              None    False       None   \n",
       "2     False    False   False              None    False       None   \n",
       "\n",
       "  num_reports distinguished subreddit_id mod_reason_by removal_reason  \\\n",
       "0        None     moderator     t5_2rm4d          None           None   \n",
       "1        None     moderator     t5_2rm4d          None           None   \n",
       "2        None          None     t5_2rm4d          None           None   \n",
       "\n",
       "  link_flair_background_color      id  is_robot_indexable report_reasons  \\\n",
       "0                              fdub8s                True           None   \n",
       "1                              fecu80                True           None   \n",
       "2                              fgwmy3                True           None   \n",
       "\n",
       "          author discussion_type  num_comments  send_replies whitelist_status  \\\n",
       "0  TheCusterWolf            None             0          True          all_ads   \n",
       "1  poppy_moonray            None             0          True          all_ads   \n",
       "2   RichardSaxon            None           202          True          all_ads   \n",
       "\n",
       "   contest_mode mod_reports  author_patreon_flair author_flair_text_color  \\\n",
       "0         False          []                 False                    None   \n",
       "1         False          []                 False                    None   \n",
       "2         False          []                 False                    None   \n",
       "\n",
       "                                           permalink parent_whitelist_status  \\\n",
       "0  /r/nosleep/comments/fdub8s/february_2020_conte...                 all_ads   \n",
       "1   /r/nosleep/comments/fecu80/january_2020_winners/                 all_ads   \n",
       "2  /r/nosleep/comments/fgwmy3/42_years_ago_we_sen...                 all_ads   \n",
       "\n",
       "   stickied                                                url  \\\n",
       "0      True                             https://redd.it/fduax3   \n",
       "1      True                             https://redd.it/fectho   \n",
       "2     False  https://www.reddit.com/r/nosleep/comments/fgwm...   \n",
       "\n",
       "   subreddit_subscribers   created_utc  num_crossposts media  is_video  \\\n",
       "0               13840304  1.583410e+09               0  None     False   \n",
       "1               13840304  1.583498e+09               0  None     False   \n",
       "2               13840304  1.583931e+09               4  None     False   \n",
       "\n",
       "  author_cakeday link_flair_template_id  \n",
       "0            NaN                    NaN  \n",
       "1           True                    NaN  \n",
       "2            NaN                    NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the 3rd layer of dict, with key 'data' for better view\n",
    "# selftext (only has value started from 3rd row) is the post text that I would like to compile for modelling\n",
    "\n",
    "df = pd.DataFrame(p['data'] for p in nosleep_dict['data']['children'])\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 101)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the total rows collected per webscrap on redditcs.com \n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Reddit will give you the top 25 posts .\n",
    "To get the next 25 posts, will need the name ID of the last post data, \n",
    "which is the key 'after' that I mentioned in previous few cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t3_fh9ag7'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the name of the last post.\n",
    "nosleep_dict['data']['after']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Collecting post from two subreddits\n",
    "\n",
    "Below is the loop function to collect more in reddits.com\n",
    "\n",
    "However, Reddit limit the number of requests per second you're allowed to make. Thus, will need to add timer to delay the loop for each requests, using `time.sleep()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Function to scrap post from Reddits.com ######\n",
    "def collect_post(url, after):\n",
    "    posts = []      # empty list to store the post after scraping\n",
    "    headers = {'User-agent': 'Appletree 8.1'}       # customer user-agent\n",
    "    \n",
    "    for i in range (4):     # number of iteration to scrap. Increase the number depending how much post to scrap\n",
    "        if after == None:      \n",
    "            params = {}          # the first 25 posts\n",
    "        else: \n",
    "            params = {'after' : after}    # name of last ID post, this is for the next 25 post scraping\n",
    "            #print('id', params)\n",
    "        \n",
    "        res = requests.get(url, params = params, headers = headers)\n",
    "        \n",
    "        if res.status_code == 200:      # check if it is okay, status_code = 200 means okay   \n",
    "            current_dict = res.json()       # if okay, use res.json() to convert the response         \n",
    "                                       # into a dictionary format and set this to a variable (current_dict)\n",
    "                    \n",
    "        # store the values from key 'data' from its 'parent key':['data']['children']\n",
    "            current_post = [p['data'] for p in current_dict['data']['children']]\n",
    "            posts.extend(current_post)  #extend save the current_post (same row), instead of as list in the posts[]\n",
    "            \n",
    "            after = current_dict['data']['after']   # ID for next 25 post\n",
    "            print('last ID:', after)\n",
    "        else:\n",
    "            print('status error!', res.status_code)\n",
    "            break\n",
    "        \n",
    "        # generate a random sleep duration to look more 'natural', instead of fix timer\n",
    "        sleep_duration = random.randint(2,10)\n",
    "        #print(sleep_duration)\n",
    "        time.sleep(sleep_duration)\n",
    "    \n",
    "    # check the number of post collected\n",
    "    print('length of collected posts:', len(posts))\n",
    "    # check the unique ID\n",
    "    print('uniqueID:', len(set([p['name'] for p in posts])))\n",
    "    \n",
    "    return posts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If want to scrape post from First reddits,\n",
    "- change `to_scrape` to **True** in below cell\n",
    "- change to **False** after scrape completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_scrape = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uncomment** below line to initiate empty post for the FIRST scrap,\n",
    "**set to comment** by adding back the `#` after the first scrap, that is, before re-run the scrape post in the next cell. Else, it will start as empty post, instead of continuing to append the post collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#posts_1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last ID: t3_fh9ag7\n",
      "last ID: t3_fglt67\n",
      "last ID: t3_fgt5sp\n",
      "last ID: t3_fgi720\n",
      "length of collected posts: 102\n",
      "uniqueID: 102\n"
     ]
    }
   ],
   "source": [
    "#### scrape post from 1st subreddits\n",
    "if to_scrape:\n",
    "    after = None    # set to 'None' for the first loop of scraping, after that use the last row of\n",
    "                    # 'last ID:' e.g: 't3_fgi720' printed out from the loop function\n",
    "        \n",
    "    url_1 = 'https://www.reddit.com/r/nosleep.json'    # url for 1st subreddit to scrape\n",
    "    scrape_1 = collect_post(url_1, after)              # call collect_post function to loop and scape from reddits\n",
    "    \n",
    "    posts_1.extend(scrape_1)\n",
    "    df_1 = pd.DataFrame(posts_1)\n",
    "    df_1.drop_duplicates(subset = 'title', inplace = True)  # drop duplicated 'title'\n",
    "    df_1.to_csv('../datasets/nosleep1.csv', index = False)   #export compiled df_1 to csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If want to scrape post from the 2nd subreddits\n",
    "- change `to_scrape` to **True** in below cell\n",
    "- change to **False** after scrape completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_scrape = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, **Uncomment** below line to initial empty post for the 2nd subreddits scrape.\n",
    "**set to comment** by adding back the `#` after the first scrape, that is, before re-run the scrape post in the next cell. Else, it will start as empty post, instead of continuing to append the post collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#posts_2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last ID: t3_ffx2x1\n",
      "last ID: t3_fejf90\n",
      "last ID: t3_fda074\n",
      "last ID: t3_fbdupg\n",
      "length of collected posts: 100\n",
      "uniqueID: 100\n"
     ]
    }
   ],
   "source": [
    "#### scrape post\n",
    "if to_scrape:\n",
    "    after = None    # set to 'None' for the first loop of scraping, after that use the last row of\n",
    "                    # 'last ID:' e.g: 't3_fbdupg' printed out from the loop function\n",
    "        \n",
    "    url_2 = 'https://www.reddit.com/r/Thetruthishere.json'    # url for 2nd subreddit to scrape\n",
    "    scrape_2 = collect_post(url_2, after)           # call collect_post function to loop and scape from reddits\n",
    "    \n",
    "    posts_2.extend(scrape_2)\n",
    "    df_2 = pd.DataFrame(posts_2)\n",
    "    df_2.drop_duplicates(subset = 'title', inplace = True)         # drop duplicated 'title'\n",
    "    pd.DataFrame(scrape_2).to_csv('../datasets/thetrueishere1.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
